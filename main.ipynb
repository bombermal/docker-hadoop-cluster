{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"SparkSample\") \\\n",
    "        .getOrCreate() # .master(\"spark://localhost:8088\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = spark.read.options(delimiter=\";\", header=True, encoding=\"latin1\", dateFormat=\"dd/MM/yyyy\", inferSchema=True).csv(\"/user/ivan/input/bweb.csv\")\n",
    "data_raw.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploração Inicial dos Dados:\n",
    "\n",
    "<ul>\n",
    "    <li>Baixar e carregar os dados para o HDFS no cluster. </li>\n",
    "    <li>Realizar uma exploração inicial dos dados, utilizando operações sobre DataFrames, como show(), count(), describe(), entre outras.</li>\n",
    "    <li>Identificar a estrutura dos registros, os tipos de dados presentes, entre outras informações relevantes</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpeza e Transformação dos Dados:\n",
    "\n",
    "<ul>\n",
    "    <li>Realizar a limpeza e transformação dos dados, eliminando registros inválidos ou inconsistentes, e convertendo os tipos de dados, se necessário.</li>\n",
    "    <li>Utilizar as transformações disponíveis em DataFrames, como select(), filter(), withColumn(), etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análise Exploratória:\n",
    "\n",
    "Realizar uma análise exploratória dos resultados das eleições, utilizando DataFrames no PySpark:<ul>\n",
    "    <li>Calcular a taxa de comparecimento às eleições por município, zona eleitoral ou seção eleitoral.</li>\n",
    "    <li>Identificar os municípios com maior e menor taxa de comparecimento.</li>\n",
    "    <li>Determinar o desempenho de cada partido, calculando a quantidade de votos recebidos por partido.</li>\n",
    "    <li>Comparar o desempenho dos partidos em diferentes municípios ou estados.</li>\n",
    "    <li>Identificar os candidatos mais votados para cada cargo.</li>\n",
    "    <li>Identificar regiões com maior concentração de votos em determinados partidos ou candidatos.</li>\n",
    "    <li>Análise da participação eleitoral por faixa etária ou gênero.</li>\n",
    "    <li>Comparar a proporção de votos brancos e nulos com o total de votos válidos.</li>\n",
    "    <li>Utilização de funções de agregação, como sum(), avg(), count(), etc., para obter insights estatísticos.</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualização de Resultados (extra)\n",
    "<ul>\n",
    "    <li>Como sugestão utilizar bibliotecas de visualização de dados, como Matplotlib ou Plotly, em conjunto com os DataFrames do PySpark, para criar gráficos e visualizações que representem os resultados das análises.</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusão:\n",
    "<ul>\n",
    "    <li>\n",
    "Destacar os insights obtidos durante a análise dos resultados das eleições</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "sc = SparkContext(appName=\"EstimatePi\")\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "NUM_SAMPLES = 1000000\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
